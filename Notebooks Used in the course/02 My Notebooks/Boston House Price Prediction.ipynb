{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cc33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38c5f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b0994b1",
   "metadata": {},
   "source": [
    "The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price. To train our machine learning model with boston housing data, we will be using scikit-learn’s boston dataset.\n",
    "\n",
    "In this dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column (price).\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d8762",
   "metadata": {},
   "source": [
    "### The Boston Housing Dataset\n",
    "\n",
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n",
    "\n",
    "CRIM per capita crime rate by town <br>\n",
    "ZN proportion of residential land zoned for lots over 25,000 sq.ft. <br>\n",
    "INDUS proportion of non-retail business acres per town <br>\n",
    "CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) <br>\n",
    "NOX nitric oxides concentration (parts per 10 million) <br>\n",
    "RM average number of rooms per dwelling <br>\n",
    "AGE proportion of owner-occupied units built prior to 1940 <br>\n",
    "DIS weighted distances to five Boston employment centres <br>\n",
    "RAD index of accessibility to radial highways <br>\n",
    "TAX full-value property-tax rate per 10,000usd <br>\n",
    "PTRATIO pupil-teacher ratio by town <br>\n",
    "B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town <br>\n",
    "LSTAT % lower status of the population <br>\n",
    "MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b06576",
   "metadata": {},
   "source": [
    "Each record in the database describes a Boston suburb or town."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3150da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets load the dataset \n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "data_check = pd.read_csv(r\"C:\\Users\\sriha\\OneDrive\\Documents\\Desktop\\JOB\\AI & ML\\ML COURSE FOR BEGINNERS FREECODECAMP YT CHANNEL\\Notebooks Used in the course\\02 My Notebooks\\housing.csv\",header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "\n",
    "data_check.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets load the dataset \n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
    "\n",
    "#Changing the name of  target variable to 'PRICE'\n",
    "# Median value of owner-occupied homes in $1000s\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\sriha\\01 ML Projects\\Untitled Folder\\housing.csv\", header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the file to back-up file\n",
    "data_bk = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72479360",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34283784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the information of the dataset\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ded05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f0530",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the unique number of values in the dataset\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531031e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See rows with missing values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, height=2.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e682d02",
   "metadata": {},
   "source": [
    "\n",
    "1. **Seaborn Pair Plot:**\n",
    "   ```python\n",
    "   sns.pairplot(data, height=2.5)\n",
    "   ```\n",
    "   - `sns.pairplot()` is a Seaborn function that creates a grid of scatterplots for all pairs of numerical columns in the DataFrame `data`.\n",
    "   - The `height=2.5` parameter adjusts the height of each subplot in the grid.\n",
    "\n",
    "2. **Matplotlib Tight Layout:**\n",
    "   ```python\n",
    "   plt.tight_layout()\n",
    "   ```\n",
    "   - `plt.tight_layout()` is a Matplotlib function that adjusts the spacing between subplots to improve the layout.\n",
    "\n",
    "In summary, this code generates a pair plot, which is a grid of scatterplots showing the relationships between pairs of numerical variables in the dataset. Each subplot in the grid represents the relationship between two variables, and the diagonal subplots display histograms for each individual variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['PRICE']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97eee5",
   "metadata": {},
   "source": [
    "1. **Seaborn Distribution Plot:**\n",
    "   ```python\n",
    "   sns.distplot(data['PRICE'])\n",
    "   ```\n",
    "   - `sns.distplot()` is a Seaborn function that combines a histogram with a kernel density estimate. It visualizes the distribution of a single variable.\n",
    "   - `data['PRICE']` selects the 'PRICE' column from the DataFrame `data`.\n",
    "\n",
    "The resulting plot provides a visual representation of the distribution of sale prices in the dataset. The histogram illustrates the frequency or density of different sale price ranges, and the kernel density estimate provides a smoothed representation of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feac2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % data['PRICE'].skew())\n",
    "print(\"Kurtosis: %f\" % data['PRICE'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47471a1",
   "metadata": {},
   "source": [
    "\n",
    "1. **Skewness Calculation:**\n",
    "   ```python\n",
    "   print(\"Skewness: %f\" % data['PRICE'].skew())\n",
    "   ```\n",
    "   - The `data['PRICE'].skew()` method calculates the skewness of the 'PRICE' column. Skewness measures the asymmetry of the distribution of values. \n",
    "   - The result is then printed using the `print` statement.\n",
    "\n",
    "2. **Kurtosis Calculation:**\n",
    "   ```python\n",
    "   print(\"Kurtosis: %f\" % data['PRICE'].kurt())\n",
    "   ```\n",
    "   - The `data['PRICE'].kurt()` method calculates the kurtosis of the 'PRICE' column. Kurtosis measures the \"tailedness\" of the distribution, indicating whether the data has heavy tails or is more peaked than a normal distribution.\n",
    "   - The result is then printed using the `print` statement.\n",
    "\n",
    "In summary, these lines of code provide insights into the shape of the distribution of PRICE. A skewness close to zero suggests a relatively symmetric distribution, while positive or negative skewness indicates skew to the right or left, respectively. Kurtosis values are compared to the normal distribution (which has a kurtosis of 3) – higher values indicate heavier tails, and lower values indicate lighter tails.\n",
    "\n",
    "\n",
    "3. **Skewness:**\n",
    "   - Skewness is a measure of the asymmetry of a distribution. \n",
    "   - If the skewness is close to 0, it indicates that the distribution is approximately symmetric.\n",
    "   - A positive skewness (greater than 0) suggests that the distribution has a longer right tail, meaning it is skewed to the right.\n",
    "   - A negative skewness (less than 0) suggests that the distribution has a longer left tail, meaning it is skewed to the left.\n",
    "\n",
    "4. **Kurtosis:**\n",
    "   - Kurtosis measures the tails and the peakedness of a distribution.\n",
    "   - A kurtosis value of 3 is often considered normal (mesokurtic) and is the kurtosis of a normal distribution.\n",
    "   - Positive kurtosis (greater than 3) indicates heavier tails and a more peaked distribution (leptokurtic).\n",
    "   - Negative kurtosis (less than 3) indicates lighter tails and a flatter distribution (platykurtic).\n",
    "\n",
    "Interpreting the results:\n",
    "- If skewness is close to 0 and kurtosis is close to 3, the distribution of 'PRICE' is approximately normal.\n",
    "- Positive skewness might suggest that there are more houses with high sale prices.\n",
    "- Positive kurtosis might suggest that the tails of the distribution are heavier, indicating more extreme values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = data['CRIM'], y = data['PRICE'])\n",
    "plt.ylabel('PRICE', fontsize=13)\n",
    "plt.xlabel('CRIM', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = data['AGE'], y = data['PRICE'])\n",
    "plt.ylabel('PRICE', fontsize=13)\n",
    "plt.xlabel('CRIM', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445551ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "sns.distplot(data['PRICE'] , fit=norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(data['PRICE'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('PRICE distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(data['PRICE'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176e7b6",
   "metadata": {},
   "source": [
    "This code performs statistical analysis and generates visualizations to examine the distribution of the 'PRICE' column in the Pandas DataFrame named `data`.\n",
    "\n",
    "1. **Importing Libraries:**\n",
    "   ```python\n",
    "   from scipy import stats\n",
    "   from scipy.stats import norm, skew\n",
    "   ```\n",
    "   - This imports the necessary functions from the SciPy library, including statistical tools (`stats`), normal distribution (`norm`), and skewness (`skew`).\n",
    "\n",
    "2. **Distribution Plot with Fitted Normal Distribution:**\n",
    "   ```python\n",
    "   sns.distplot(data['PRICE'], fit=norm);\n",
    "   ```\n",
    "   - The Seaborn `distplot` function is used to create a histogram of the 'PRICE' distribution.\n",
    "   - The `fit=norm` parameter fits a normal distribution to the data and overlays it on the histogram.\n",
    "\n",
    "3. **Calculating Mean and Standard Deviation:**\n",
    "   ```python\n",
    "   (mu, sigma) = norm.fit(data['PRICE'])\n",
    "   print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "   ```\n",
    "   - The mean (`mu`) and standard deviation (`sigma`) of the 'PRICE' distribution are calculated using the `norm.fit` function.\n",
    "   - These values are then printed to the console.\n",
    "\n",
    "4. **Legend and Plot Customization:**\n",
    "   ```python\n",
    "   plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "   plt.ylabel('Frequency')\n",
    "   plt.title('PRICE distribution')\n",
    "   ```\n",
    "   - A legend is added to the plot, indicating the parameters of the fitted normal distribution.\n",
    "   - The y-axis is labeled as 'Frequency,' and the title of the plot is set to 'PRICE distribution.'\n",
    "\n",
    "5. **QQ-Plot (Quantile-Quantile Plot):**\n",
    "   ```python\n",
    "   fig = plt.figure()\n",
    "   res = stats.probplot(data['PRICE'], plot=plt)\n",
    "   plt.show()\n",
    "   ```\n",
    "   - A Quantile-Quantile plot (QQ-plot) is created using the `stats.probplot` function from SciPy.\n",
    "   - The QQ-plot compares the quantiles of the 'PRICE' distribution to the quantiles of a theoretical normal distribution.\n",
    "\n",
    "In summary, this code aims to analyze and visualize the distribution of 'PRICE,' checking whether it follows a normal distribution. The histogram with a fitted normal distribution provides a visual comparison, and the QQ-plot further assesses the normality assumption. The mean and standard deviation are also calculated and displayed.\n",
    "\n",
    "\n",
    "\n",
    "6. **Histogram with Fitted Normal Distribution:**\n",
    "   - The `sns.distplot()` function generates a histogram of the 'PRICE' distribution. The `fit=norm` parameter overlays a fitted normal distribution on the histogram.\n",
    "   - This visualization helps in assessing how closely the actual distribution aligns with a normal distribution.\n",
    "\n",
    "7. **Mean and Standard Deviation Calculation:**\n",
    "   - The mean (`mu`) and standard deviation (`sigma`) of the 'PRICE' distribution are calculated using the `norm.fit` function.\n",
    "   - These statistical measures provide key summary statistics for understanding the central tendency and spread of the data.\n",
    "\n",
    "8. **Legend and Plot Customization:**\n",
    "   - A legend is added to the plot, providing information about the parameters of the fitted normal distribution (mean and standard deviation).\n",
    "   - The y-axis is labeled as 'Frequency,' and the title of the plot is set to 'PRICE distribution.'\n",
    "   - These elements enhance the interpretability of the plot.\n",
    "\n",
    "9. **Quantile-Quantile (QQ) Plot:**\n",
    "   - The QQ-plot is created using the `stats.probplot` function. It compares the quantiles of the observed 'PRICE' distribution to the quantiles of a theoretical normal distribution.\n",
    "   - A straight line in the QQ-plot suggests that the data follows a normal distribution. Deviations from the line indicate departures from normality.\n",
    "\n",
    "The combined use of the histogram, fitted normal distribution, and QQ-plot allows for a comprehensive examination of the 'PRICE' distribution. Deviations from normality might suggest the need for data transformation or consideration of alternative statistical approaches.\n",
    "\n",
    "Performing statistical analysis and visualizations on the target variable, such as 'PRICE' in this case, is a crucial step in the machine learning (ML) process.\n",
    "\n",
    "1. **Understanding Data Distribution:**\n",
    "   - Analyzing the distribution of the target variable helps you understand its underlying patterns and characteristics. This understanding is essential for making informed decisions throughout the ML process.\n",
    "\n",
    "2. **Normality Assumption:**\n",
    "   - Many machine learning algorithms assume that the target variable follows a normal distribution. By visualizing the distribution and comparing it to a normal distribution, you can assess whether this assumption holds.\n",
    "\n",
    "3. **Identifying Skewness:**\n",
    "   - Skewness, a measure of asymmetry in the distribution, can impact the performance of certain algorithms. Identifying and addressing skewness (if present) through transformations or other techniques can improve model accuracy.\n",
    "\n",
    "4. **Outlier Detection:**\n",
    "   - Visualizations, such as the QQ-plot, help in identifying outliers in the target variable. Outliers can have a significant impact on the model, and their detection allows for consideration of appropriate handling strategies.\n",
    "\n",
    "5. **Feature Engineering:**\n",
    "   - Understanding the statistical properties of the target variable may guide feature engineering decisions. For example, transformations like log transformations might be applied to achieve a more symmetric distribution.\n",
    "\n",
    "6. **Model Performance:**\n",
    "   - The distribution and statistical properties of the target variable can influence the choice of appropriate modeling techniques. Some algorithms work well with normally distributed data, while others are more robust to deviations from normality.\n",
    "\n",
    "7. **Interpretability and Communication:**\n",
    "   - Visualizations, such as the histogram and QQ-plot, provide interpretable insights into the target variable's behavior. Communicating these insights to stakeholders is crucial for collaborative decision-making.\n",
    "\n",
    "8. **Data Preprocessing Decisions:**\n",
    "   - Findings from the analysis may drive preprocessing decisions, such as handling missing values, imputing outliers, or selecting appropriate transformation techniques.\n",
    "\n",
    "In summary, the analysis and visualizations performed on the target variable contribute to making informed decisions at various stages of the ML process. They guide preprocessing steps, model selection, and help ensure that the chosen algorithms align with the characteristics of the data. This, in turn, contributes to the development of accurate and robust machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbac851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"PRICE\"] = np.log1p(data[\"PRICE\"])\n",
    "\n",
    "sns.distplot(data['PRICE'] , fit=norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(data['PRICE'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('PRICE distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(data['PRICE'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25ee86",
   "metadata": {},
   "source": [
    "This code performs a log transformation on the 'PRICE' column in the Pandas DataFrame named `data` and then visualizes the transformed distribution through statistical analysis\n",
    "\n",
    "1. **Log Transformation:**\n",
    "   ```python\n",
    "   data[\"PRICE\"] = np.log1p(data[\"PRICE\"])\n",
    "   ```\n",
    "   - This line applies a log transformation to the 'SalePrice' column using `np.log1p`. Log transformations are often used to address skewness in the data and stabilize variances.\n",
    "\n",
    "2. **Distribution Plot with Fitted Normal Distribution (After Transformation):**\n",
    "   ```python\n",
    "   sns.distplot(data['PRICE'], fit=norm);\n",
    "   ```\n",
    "   - The Seaborn `distplot` function creates a histogram of the log-transformed 'PRICE' distribution.\n",
    "   - The `fit=norm` parameter fits a normal distribution to the transformed data and overlays it on the histogram.\n",
    "\n",
    "3. **Calculating Mean and Standard Deviation (After Transformation):**\n",
    "   ```python\n",
    "   (mu, sigma) = norm.fit(data['PRICE'])\n",
    "   print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "   ```\n",
    "   - The mean (`mu`) and standard deviation (`sigma`) of the log-transformed 'PRICE' distribution are calculated using the `norm.fit` function.\n",
    "   - These values are then printed to the console.\n",
    "\n",
    "4. **Legend and Plot Customization (After Transformation):**\n",
    "   ```python\n",
    "   plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "   plt.ylabel('Frequency')\n",
    "   plt.title('PRICE distribution')\n",
    "   ```\n",
    "   - A legend is added to the plot, indicating the parameters of the fitted normal distribution for the log-transformed data.\n",
    "   - The y-axis is labeled as 'Frequency,' and the title of the plot is set to 'PRICE distribution.'\n",
    "\n",
    "5. **QQ-Plot (Quantile-Quantile Plot) After Transformation:**\n",
    "   ```python\n",
    "   fig = plt.figure()\n",
    "   res = stats.probplot(data['PRICE'], plot=plt)\n",
    "   plt.show()\n",
    "   ```\n",
    "   - A QQ-plot is created using the `stats.probplot` function for the log-transformed 'PRICE' distribution.\n",
    "   - This plot assesses how well the transformed data aligns with a theoretical normal distribution.\n",
    "\n",
    "In summary, this code performs a log transformation on the 'PRICE' column and then visualizes the distribution of the transformed data. The log transformation is applied to address skewness, and the subsequent analysis checks for improvements in normality and provides insights into the statistical properties of the transformed variable.\n",
    "\n",
    "The log transformation applied to the 'PRICE' column in the machine learning (ML) process serves several purposes and can bring benefits to the analysis:\n",
    "\n",
    "1. **Skewness Correction:**\n",
    "   - The log transformation is often used to mitigate skewness in the distribution of a variable. Skewed distributions can negatively impact the performance of some machine learning algorithms that assume normality or work better with symmetric data. By applying the log transformation, the distribution becomes more symmetrical.\n",
    "\n",
    "2. **Homoscedasticity Improvement:**\n",
    "   - Homoscedasticity, which refers to constant variance across the range of the target variable, is an assumption in many regression models. The log transformation can stabilize the variance, particularly when the variance of the variable increases with its level. This can lead to more consistent model performance.\n",
    "\n",
    "3. **Model Sensitivity Reduction:**\n",
    "   - Some machine learning models, such as linear regression, are sensitive to the scale and distribution of the target variable. Transformations like the log can reduce the impact of extreme values and outliers, making the model more robust.\n",
    "\n",
    "4. **Improving Linearity:**\n",
    "   - Linear models assume a linear relationship between predictors and the target variable. The log transformation can help in achieving a more linear relationship, especially when the target variable exhibits exponential growth.\n",
    "\n",
    "5. **Handling Multiplicative Effects:**\n",
    "   - In certain situations where the relationship between predictors and the target variable is multiplicative rather than additive, the log transformation can convert the multiplicative relationship into an additive one, making it more suitable for linear models.\n",
    "\n",
    "6. **Interpretability Enhancement:**\n",
    "   - Log transformations can improve the interpretability of the model coefficients. For example, in the context of house prices, a log transformation may correspond to a percentage change in price, which can be more interpretable than a raw price change.\n",
    "\n",
    "7. **Normality Assumption:**\n",
    "   - Some algorithms assume that the target variable follows a normal distribution. While the log transformation doesn't guarantee normality, it often helps in making the distribution more normal or approximately normal.\n",
    "\n",
    "It's important to note that the decision to perform a log transformation depends on the characteristics of the data and the specific requirements of the modeling task. Experimentation and validation are key to determining whether such transformations contribute to the overall improvement of the machine learning model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66e310",
   "metadata": {},
   "source": [
    "# Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62958bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the correlation between the features\n",
    "corr = data.corr()\n",
    "corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91745b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.PuBuGn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4db46",
   "metadata": {},
   "source": [
    "# Reference - 1\n",
    "\n",
    "The provided code generates a heatmap of the correlation between features in the Pandas DataFrame named `data` using the Seaborn and Matplotlib libraries.\n",
    "\n",
    "```python\n",
    "# Plotting the heatmap of correlation between features\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')\n",
    "```\n",
    "\n",
    "- This code creates a heatmap using Seaborn's `heatmap` function. Here's a breakdown of the parameters:\n",
    "\n",
    "  - `plt.figure(figsize=(20,20))`: Sets the size of the Matplotlib figure to 20x20 inches.\n",
    "\n",
    "  - `sns.heatmap(corr, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')`:\n",
    "    - `corr`: The correlation matrix calculated earlier.\n",
    "    - `cbar=True`: Displays the colorbar on the side of the heatmap.\n",
    "    - `square=True`: Ensures that the heatmap is square-shaped.\n",
    "    - `fmt='.1f'`: Formats the numbers in the heatmap to have one decimal place.\n",
    "    - `annot=True`: Displays the correlation values in each cell of the heatmap.\n",
    "    - `annot_kws={'size':15}`: Adjusts the font size of the annotation text to 15.\n",
    "    - `cmap='Greens'`: Specifies the color map to be used for the heatmap (in this case, shades of green).\n",
    "\n",
    "The resulting heatmap visually represents the correlation matrix, where each cell's color intensity corresponds to the strength and direction of the correlation between the corresponding pair of features. This visualization is useful for identifying patterns and relationships within the dataset.\n",
    "\n",
    "The heatmap of correlation between features plays a significant role in the machine learning (ML) process, especially during the exploratory data analysis (EDA) phase and feature selection. Here's how it is relevant:\n",
    "\n",
    "1. **Feature Relationships:**\n",
    "   - The heatmap visually represents the correlation between different features in the dataset. It helps identify which features have strong positive or negative correlations, providing insights into potential relationships between variables.\n",
    "\n",
    "2. **Multicollinearity Detection:**\n",
    "   - High correlations between features may indicate multicollinearity, where two or more features are highly correlated with each other. Multicollinearity can affect the performance of certain ML models, especially linear regression, as it assumes independence between features.\n",
    "\n",
    "3. **Feature Selection:**\n",
    "   - Understanding feature correlations is crucial for feature selection. If two features are highly correlated, one of them may be redundant, and removing one can simplify the model without sacrificing much information. This is especially relevant in cases where having too many features can lead to overfitting.\n",
    "\n",
    "4. **Model Performance:**\n",
    "   - Correlation analysis can provide insights into which features might be more influential in predicting the target variable. ML models benefit from relevant features that are not highly correlated with each other, leading to better generalization on new data.\n",
    "\n",
    "5. **Visualization for Interpretability:**\n",
    "   - Heatmaps offer an intuitive and visual representation of correlations, making it easier for analysts, data scientists, and stakeholders to interpret the relationships within the dataset.\n",
    "\n",
    "6. **Identifying Patterns:**\n",
    "   - Patterns in the correlation matrix can reveal interesting insights. For example, a strong negative correlation between two features might indicate an inverse relationship, providing valuable information for understanding the data.\n",
    "\n",
    "7. **Preprocessing Decisions:**\n",
    "   - Correlation analysis can influence preprocessing decisions. For instance, if there's a high correlation between two features, you might choose to keep only one of them to simplify the model and reduce the risk of overfitting.\n",
    "\n",
    "In summary, the heatmap of correlation is a valuable tool in the ML process for understanding feature relationships, detecting multicollinearity, aiding in feature selection, and making informed decisions during data preprocessing. It contributes to building more effective and interpretable machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11811bdb",
   "metadata": {},
   "source": [
    "# Reference-2\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cmaps = [('Perceptually Uniform Sequential', [\n",
    "            'viridis', 'plasma', 'inferno', 'magma', 'cividis']),\n",
    "         ('Sequential', [\n",
    "            'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']),\n",
    "         ('Sequential (2)', [\n",
    "            'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink',\n",
    "            'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia',\n",
    "            'hot', 'afmhot', 'gist_heat', 'copper']),\n",
    "         ('Diverging', [\n",
    "            'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu',\n",
    "            'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']),\n",
    "         ('Cyclic', ['twilight', 'twilight_shifted', 'hsv']),\n",
    "         ('Qualitative', [\n",
    "            'Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "            'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "            'tab10', 'tab20', 'tab20b', 'tab20c']),\n",
    "         ('Miscellaneous', [\n",
    "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
    "            'gist_rainbow', 'rainbow', 'jet', 'turbo', 'nipy_spectral',\n",
    "            'gist_ncar'])]\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "def plot_color_gradients(cmap_category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows-1)*0.1)*0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n",
    "    fig.subplots_adjust(top=1-.35/figh, bottom=.15/figh, left=0.2, right=0.99)\n",
    "\n",
    "    axs[0].set_title(f\"{cmap_category} colormaps\", fontsize=14)\n",
    "\n",
    "    for ax, cmap_name in zip(axs, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=cmap_name)\n",
    "        ax.text(-.01, .5, cmap_name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axs:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "for cmap_category, cmap_list in cmaps:\n",
    "    plot_color_gradients(cmap_category, cmap_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmaps = [('Perceptually Uniform Sequential', [\n",
    "            'viridis', 'plasma', 'inferno', 'magma', 'cividis']),\n",
    "         ('Sequential', [\n",
    "            'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']),\n",
    "         ('Sequential (2)', [\n",
    "            'binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink',\n",
    "            'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia',\n",
    "            'hot', 'afmhot', 'gist_heat', 'copper']),\n",
    "         ('Diverging', [\n",
    "            'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu',\n",
    "            'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']),\n",
    "         ('Cyclic', ['twilight', 'twilight_shifted', 'hsv']),\n",
    "         ('Qualitative', [\n",
    "            'Pastel1', 'Pastel2', 'Paired', 'Accent',\n",
    "            'Dark2', 'Set1', 'Set2', 'Set3',\n",
    "            'tab10', 'tab20', 'tab20b', 'tab20c']),\n",
    "         ('Miscellaneous', [\n",
    "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg',\n",
    "            'gist_rainbow', 'rainbow', 'jet', 'turbo', 'nipy_spectral',\n",
    "            'gist_ncar'])]\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "def plot_color_gradients(cmap_category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows-1)*0.1)*0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n",
    "    fig.subplots_adjust(top=1-.35/figh, bottom=.15/figh, left=0.2, right=0.99)\n",
    "\n",
    "    axs[0].set_title(f\"{cmap_category} colormaps\", fontsize=14)\n",
    "\n",
    "    for ax, cmap_name in zip(axs, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=cmap_name)\n",
    "        ax.text(-.01, .5, cmap_name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axs:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "for cmap_category, cmap_list in cmaps:\n",
    "    plot_color_gradients(cmap_category, cmap_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a9e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_target = abs(corr[\"PRICE\"]) # absolute value of the correlation \n",
    "\n",
    "relevant_features = cor_target[cor_target>0.2] # highly correlated features \n",
    "\n",
    "names = [index for index, value in relevant_features.iteritems()] # getting the names of the features \n",
    "\n",
    "names.remove('PRICE') # removing target feature \n",
    "\n",
    "print(names) # printing the features \n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331b684",
   "metadata": {},
   "source": [
    "This code performs feature selection based on the absolute correlation coefficients between the features and the target variable ('PRICE').\n",
    "\n",
    "1. **Calculating Absolute Correlation:**\n",
    "   ```python\n",
    "   cor_target = abs(cor[\"PRICE\"])\n",
    "   ```\n",
    "   - This line calculates the absolute correlation coefficients between each feature and the target variable ('PRICE').\n",
    "\n",
    "2. **Selecting Highly Correlated Features:**\n",
    "   ```python\n",
    "   relevant_features = cor_target[cor_target > 0.2]\n",
    "   ```\n",
    "   - The code selects features that have an absolute correlation coefficient greater than 0.2 with the target variable. The threshold of 0.2 is chosen to identify features that have a relatively strong correlation with the target.\n",
    "\n",
    "3. **Getting Feature Names:**\n",
    "   ```python\n",
    "   names = [index for index, value in relevant_features.iteritems()]\n",
    "   ```\n",
    "   - This line extracts the names of the features that meet the correlation threshold. It uses a list comprehension to iterate over the items in the `relevant_features` series and retrieves the feature names.\n",
    "\n",
    "4. **Removing Target Feature:**\n",
    "   ```python\n",
    "   names.remove('PRICE')\n",
    "   ```\n",
    "   - The code removes the target feature ('PRICE') from the list of selected feature names since the target itself is not considered as a predictor.\n",
    "\n",
    "5. **Printing Selected Features and Count:**\n",
    "   ```python\n",
    "   print(names)\n",
    "   print(len(names))\n",
    "   ```\n",
    "   - Finally, the code prints the names of the selected features and the count of features selected based on the correlation threshold.\n",
    "\n",
    "In summary, this code is a feature selection step that identifies features with a relatively strong absolute correlation with the target variable ('PRICE'). The selected features are printed, and the count of selected features is also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9545cf8",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcfeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the independent and Target variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in data.columns:\n",
    "    if col != 'PRICE':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'PRICE'\n",
    "\n",
    "x = data[IndepVar]\n",
    "y = data[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1031f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ad0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Display the shape of the train_data and test_data\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c3830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a16f5860",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5784a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the result dataset\n",
    "\n",
    "RGRResults_train = pd.read_csv(r\"C:\\Users\\sriha\\OneDrive\\Documents\\Desktop\\JOB\\AI & ML\\ML COURSE FOR BEGINNERS FREECODECAMP YT CHANNEL\\Notebooks Used in the course\\02 My Notebooks\\RGRResults_train.csv\", header=0)\n",
    "\n",
    "RGRResults_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f549d",
   "metadata": {},
   "source": [
    "# For train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc973d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the multi regression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression  \n",
    "\n",
    "# Create object for the model\n",
    "\n",
    "ModelMLR = LinearRegression()\n",
    "\n",
    "# Train the model with training data\n",
    "\n",
    "ModelMLR.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with train dataset\n",
    "\n",
    "y_pred = ModelMLR.predict(x_train)\n",
    "\n",
    "# Evaluation metrics for Regression analysis\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error (MAE):', round(metrics.mean_absolute_error(y_train, y_pred),3))  \n",
    "print('Mean Squared Error (MSE):', round(metrics.mean_squared_error(y_train, y_pred),3))  \n",
    "print('Root Mean Squared Error (RMSE):', round(np.sqrt(metrics.mean_squared_error(y_train, y_pred)),3))\n",
    "print('R2_score:', round(metrics.r2_score(y_train, y_pred),6))\n",
    "#print('Root Mean Squared Log Error (RMSLE):', round(np.log(np.sqrt(metrics.mean_squared_error(y_train, y_pred))),3))\n",
    "\n",
    "# Define the function to calculate the MAPE - Mean Absolute Percentage Error\n",
    "\n",
    "def MAPE (y_train, y_pred): \n",
    "    y_train, y_pred = np.array(y_train), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_train - y_pred) / y_train)) * 100\n",
    "\n",
    "# Evaluation of MAPE \n",
    "\n",
    "result = MAPE(y_train, y_pred)\n",
    "print('Mean Absolute Percentage Error (MAPE):', round(result, 3), '%')\n",
    "\n",
    "# Calculate Adjusted R squared values \n",
    "\n",
    "r_squared = round(metrics.r2_score(y_train, y_pred),6)\n",
    "adjusted_r_squared = round(1 - (1-r_squared)*(len(y)-1)/(len(y)-x.shape[1]-1),6)\n",
    "print('Adj R Square: ', adjusted_r_squared)\n",
    "print('------------------------------------------------------------------------------------------------------------')\n",
    "#-------------------------------------------------------------------------------------------\n",
    "new_row = {'Model Name' : ModelMLR,\n",
    "               'Mean_Absolute_Error_MAE' : metrics.mean_absolute_error(y_train, y_pred),\n",
    "               'Adj_R_Square' : adjusted_r_squared,\n",
    "               'Root_Mean_Squared_Error_RMSE' : np.sqrt(metrics.mean_squared_error(y_train, y_pred)),\n",
    "               'Mean_Absolute_Percentage_Error_MAPE' : result,\n",
    "               'Mean_Squared_Error_MSE' : metrics.mean_squared_error(y_train, y_pred),\n",
    "               'Root_Mean_Squared_Log_Error_RMSLE': np.log(np.sqrt(metrics.mean_squared_error(y_train, y_pred))),\n",
    "               'R2_score' : metrics.r2_score(y_train, y_pred)}\n",
    "RGRResults_train = RGRResults_train.append(new_row, ignore_index=True)\n",
    "#-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe76ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGRResults_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7caf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.DataFrame({'PRICE_A':y_train, 'PRICE_P':y_pred})\n",
    "\n",
    "# Merge two Dataframes on index of both the dataframes\n",
    "\n",
    "ResultsFinal = data.merge(Results, left_index=True, right_index=True)\n",
    "ResultsFinal.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91935c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the %of Error\n",
    "\n",
    "ResultsFinal['%Error'] = round(((ResultsFinal['PRICE_A']-ResultsFinal['PRICE_P'])/ResultsFinal['PRICE_A'])*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "\n",
    "ResultsFinal.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the differences between actual prices and predicted values\n",
    "plt.scatter(y_train, y_pred)\n",
    "plt.xlabel(\"Prices\")\n",
    "plt.ylabel(\"Predicted prices\")\n",
    "plt.title(\"Prices vs Predicted prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking residuals\n",
    "plt.scatter(y_pred,y_train-y_pred)\n",
    "plt.title(\"Predicted vs residuals\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Normality of errors\n",
    "sns.distplot(y_train-y_pred)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Regression / Regressor models\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create objects of Regression / Regressor models with default hyper-parameters\n",
    "\n",
    "ModelDCR = DecisionTreeRegressor()\n",
    "ModelRFR = RandomForestRegressor()\n",
    "ModelETR = ExtraTreesRegressor()\n",
    "ModelBRR = BayesianRidge()\n",
    "\n",
    "ModelSVR = SVR()\n",
    "modelXGR = xgb.XGBRegressor()\n",
    "ModelKNN = KNeighborsRegressor(n_neighbors=5)\n",
    "modelBRR = BayesianRidge()\n",
    "modelBGR = BaggingRegressor()\n",
    "modelGBR = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0,\n",
    "                                     criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "                                     min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "                                     init=None, random_state=None, max_features=None,\n",
    "                                     alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False,\n",
    "                                     validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "\n",
    "# Evalution matrix for all the algorithms\n",
    "\n",
    "#MM = [modelmlg, modeldcr, modelrfr, modelSVR, modelXGR, modelKNN, modelETR, modelBRR, modelBGR, modelGBR]\n",
    "MM = [ModelDCR, ModelRFR, ModelETR, ModelBRR, ModelSVR,modelXGR,ModelKNN,modelBRR,modelBGR,modelGBR]\n",
    "\n",
    "for models in MM:\n",
    "    \n",
    "    # Fit the model with train data\n",
    "    \n",
    "    models.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict the model with train dataset\n",
    "\n",
    "    y_pred = models.predict(x_train)\n",
    "    \n",
    "    # Print the model name\n",
    "    \n",
    "    print('Model Name: ', models)\n",
    "    \n",
    "    # Evaluation metrics for Regression analysis\n",
    "\n",
    "    from sklearn import metrics\n",
    "\n",
    "    print('Mean Absolute Error (MAE):', round(metrics.mean_absolute_error(y_train, y_pred),3))  \n",
    "    print('Mean Squared Error (MSE):', round(metrics.mean_squared_error(y_train, y_pred),3))  \n",
    "    print('Root Mean Squared Error (RMSE):', round(np.sqrt(metrics.mean_squared_error(y_train, y_pred)),3))\n",
    "    print('R2_score:', round(metrics.r2_score(y_train, y_pred),6))\n",
    "    #print('Root Mean Squared Log Error (RMSLE):', round(np.log(np.sqrt(metrics.mean_squared_error(y_train, y_pred))),3))\n",
    "\n",
    "    # Define the function to calculate the MAPE - Mean Absolute Percentage Error\n",
    "\n",
    "    def MAPE (y_train, y_pred): \n",
    "        y_train, y_pred = np.array(y_train), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_train - y_pred) / y_train)) * 100\n",
    "\n",
    "    # Evaluation of MAPE \n",
    "\n",
    "    result = MAPE(y_train, y_pred)\n",
    "    print('Mean Absolute Percentage Error (MAPE):', round(result, 3), '%')\n",
    "\n",
    "    # Calculate Adjusted R squared values \n",
    "\n",
    "    r_squared = round(metrics.r2_score(y_train, y_pred),6)\n",
    "    adjusted_r_squared = round(1 - (1-r_squared)*(len(y)-1)/(len(y)-x.shape[1]-1),6)\n",
    "    print('Adj R Square: ', adjusted_r_squared)\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    new_row = {'Model Name' : models,\n",
    "                   'Mean_Absolute_Error_MAE' : metrics.mean_absolute_error(y_train, y_pred),\n",
    "                   'Adj_R_Square' : adjusted_r_squared,\n",
    "                   'Root_Mean_Squared_Error_RMSE' : np.sqrt(metrics.mean_squared_error(y_train, y_pred)),\n",
    "                   'Mean_Absolute_Percentage_Error_MAPE' : result,\n",
    "                   'Mean_Squared_Error_MSE' : metrics.mean_squared_error(y_train, y_pred),\n",
    "                   'Root_Mean_Squared_Log_Error_RMSLE': np.log(np.sqrt(metrics.mean_squared_error(y_train, y_pred))),\n",
    "                   'R2_score' : metrics.r2_score(y_train, y_pred)}\n",
    "    RGRResults_train = RGRResults_train.append(new_row, ignore_index=True)\n",
    "    #-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGRResults_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4740d13",
   "metadata": {},
   "source": [
    "# For Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the result dataset\n",
    "\n",
    "RGRResults_test = pd.read_csv(r\"C:\\Users\\sriha\\OneDrive\\Documents\\Desktop\\JOB\\AI & ML\\ML COURSE FOR BEGINNERS FREECODECAMP YT CHANNEL\\Notebooks Used in the course\\02 My Notebooks\\RGRResults_test.csv\", header=0)\n",
    "\n",
    "RGRResults_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the multi regression model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression  \n",
    "\n",
    "# Create object for the model\n",
    "\n",
    "ModelMLR = LinearRegression()\n",
    "\n",
    "# Train the model with training data\n",
    "\n",
    "ModelMLR.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test dataset\n",
    "\n",
    "y_pred = ModelMLR.predict(x_test)\n",
    "\n",
    "# Evaluation metrics for Regression analysis\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error (MAE):', round(metrics.mean_absolute_error(y_test, y_pred),3))  \n",
    "print('Mean Squared Error (MSE):', round(metrics.mean_squared_error(y_test, y_pred),3))  \n",
    "print('Root Mean Squared Error (RMSE):', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3))\n",
    "print('R2_score:', round(metrics.r2_score(y_test, y_pred),6))\n",
    "#print('Root Mean Squared Log Error (RMSLE):', round(np.log(np.sqrt(metrics.mean_squared_error(y_test, y_pred))),3))\n",
    "print('Mean Absolute Percentage Error (MAPE):', round(metrics.mean_absolute_percentage_error(y_test, y_pred)*100,3), '%')\n",
    "# Define the function to calculate the MAPE - Mean Absolute Percentage Error\n",
    "\n",
    "def MAPE (y_test, y_pred): \n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# Evaluation of MAPE \n",
    "\n",
    "result = MAPE(y_test, y_pred)\n",
    "print('Mean Absolute Percentage Error (MAPE):', round(result, 3), '%')\n",
    "\n",
    "# Calculate Adjusted R squared values \n",
    "\n",
    "r_squared = round(metrics.r2_score(y_test, y_pred),6)\n",
    "adjusted_r_squared = round(1 - (1-r_squared)*(len(y)-1)/(len(y)-x.shape[1]-1),6)\n",
    "print('Adj R Square: ', adjusted_r_squared)\n",
    "print('------------------------------------------------------------------------------------------------------------')\n",
    "#-------------------------------------------------------------------------------------------\n",
    "new_row = {'Model Name' : ModelMLR,\n",
    "               'Mean_Absolute_Error_MAE' : metrics.mean_absolute_error(y_test, y_pred),\n",
    "               'Adj_R_Square' : adjusted_r_squared,\n",
    "               'Root_Mean_Squared_Error_RMSE' : np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "               'Mean_Absolute_Percentage_Error_MAPE' : result,\n",
    "               'Mean_Squared_Error_MSE' : metrics.mean_squared_error(y_test, y_pred),\n",
    "               'Root_Mean_Squared_Log_Error_RMSLE': np.log(np.sqrt(metrics.mean_squared_error(y_test, y_pred))),\n",
    "               'R2_score' : metrics.r2_score(y_test, y_pred)}\n",
    "RGRResults_test = RGRResults_test.append(new_row, ignore_index=True)\n",
    "#-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80758f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGRResults_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.DataFrame({'PRICE_A':y_test, 'PRICE_P':y_pred})\n",
    "\n",
    "# Merge two Dataframes on index of both the dataframes\n",
    "\n",
    "ResultsFinal = data.merge(Results, left_index=True, right_index=True)\n",
    "ResultsFinal.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsFinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the %of Error\n",
    "\n",
    "ResultsFinal['%Error'] = round(((ResultsFinal['PRICE_A']-ResultsFinal['PRICE_P'])/ResultsFinal['PRICE_A'])*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "\n",
    "ResultsFinal.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the differences between actual prices and predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Prices\")\n",
    "plt.ylabel(\"Predicted prices\")\n",
    "plt.title(\"Prices vs Predicted prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebee00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking residuals\n",
    "plt.scatter(y_pred,y_test-y_pred)\n",
    "plt.title(\"Predicted vs residuals\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Normality of errors\n",
    "sns.distplot(y_test-y_pred)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d292e0",
   "metadata": {},
   "source": [
    "### Comparision with different Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65678dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Regression / Regressor models\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create objects of Regression / Regressor models with default hyper-parameters\n",
    "\n",
    "ModelDCR = DecisionTreeRegressor()\n",
    "ModelRFR = RandomForestRegressor()\n",
    "ModelETR = ExtraTreesRegressor()\n",
    "ModelBRR = BayesianRidge()\n",
    "\n",
    "ModelSVR = SVR()\n",
    "modelXGR = xgb.XGBRegressor()\n",
    "ModelKNN = KNeighborsRegressor(n_neighbors=5)\n",
    "modelBRR = BayesianRidge()\n",
    "modelBGR = BaggingRegressor()\n",
    "modelGBR = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0,\n",
    "                                     criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "                                     min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "                                     init=None, random_state=None, max_features=None,\n",
    "                                     alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False,\n",
    "                                     validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "\n",
    "# Evalution matrix for all the algorithms\n",
    "\n",
    "#MM = [modelmlg, modeldcr, modelrfr, modelSVR, modelXGR, modelKNN, modelETR, modelBRR, modelBGR, modelGBR]\n",
    "MM = [ModelDCR, ModelRFR, ModelETR, ModelBRR, ModelSVR,modelXGR,ModelKNN,modelBRR,modelBGR,modelGBR]\n",
    "\n",
    "for models in MM:\n",
    "    \n",
    "    # Fit the model with train data\n",
    "    \n",
    "    models.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict the model with test data\n",
    "\n",
    "    y_pred = models.predict(x_test)\n",
    "    \n",
    "    # Print the model name\n",
    "    \n",
    "    print('Model Name: ', models)\n",
    "    \n",
    "    # Evaluation metrics for Regression analysis\n",
    "\n",
    "    from sklearn import metrics\n",
    "\n",
    "    print('Mean Absolute Error (MAE):', round(metrics.mean_absolute_error(y_test, y_pred),3))  \n",
    "    print('Mean Squared Error (MSE):', round(metrics.mean_squared_error(y_test, y_pred),3))  \n",
    "    print('Root Mean Squared Error (RMSE):', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),3))\n",
    "    print('R2_score:', round(metrics.r2_score(y_test, y_pred),6))\n",
    "    print('Root Mean Squared Log Error (RMSLE):', round(np.log(np.sqrt(metrics.mean_squared_error(y_test, y_pred))),3))\n",
    "    \n",
    "    # Define the function to calculate the MAPE - Mean Absolute Percentage Error\n",
    "\n",
    "    def MAPE (y_test, y_pred):\n",
    "        y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "    # Evaluation of MAPE \n",
    "\n",
    "    result = MAPE(y_test, y_pred)\n",
    "    print('Mean Absolute Percentage Error (MAPE):', round(result, 2), '%')\n",
    "    \n",
    "    # Calculate Adjusted R squared values \n",
    "\n",
    "    r_squared = round(metrics.r2_score(y_test, y_pred),6)\n",
    "    adjusted_r_squared = round(1 - (1-r_squared)*(len(y)-1)/(len(y)-x.shape[1]-1),6)\n",
    "    print('Adj R Square: ', adjusted_r_squared)\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    new_row = {'Model Name' : models,\n",
    "               'Mean_Absolute_Error_MAE' : metrics.mean_absolute_error(y_test, y_pred),\n",
    "               'Adj_R_Square' : adjusted_r_squared,\n",
    "               'Root_Mean_Squared_Error_RMSE' : np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "               'Mean_Absolute_Percentage_Error_MAPE' : result,\n",
    "               'Mean_Squared_Error_MSE' : metrics.mean_squared_error(y_test, y_pred),\n",
    "               'Root_Mean_Squared_Log_Error_RMSLE': np.log(np.sqrt(metrics.mean_squared_error(y_test, y_pred))),\n",
    "               'R2_score' : metrics.r2_score(y_test, y_pred)}\n",
    "    RGRResults_test = RGRResults_test.append(new_row, ignore_index=True)\n",
    "    #-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGRResults_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
